{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_rJPb9JyvJy",
        "outputId": "4c15908c-752e-4127-cd7c-e14498e07dff"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/hahnicity/ventmode\n",
        "!python3 -m pip install ./ventmode\n",
        "!python -m pip install git+https://github.com/lukauskas/dtwco.git#egg=dtwco\n",
        "%pip install tslearn visualkeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TE99YUVyxXR",
        "outputId": "154ea94c-7fd2-4711-e518-b2dbfaad43bb"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Villiamfj/ventTools\n",
        "%pip install -e ventTools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPSeM8e7gCDO"
      },
      "outputs": [],
      "source": [
        "%pip install -q -U keras-tuner\n",
        "import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "lc3Bda_yDII-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ventTools\n",
        "from ventTools.data import convert_to_fixed_length\n",
        "from ast import literal_eval\n",
        "import ventmode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzoAvdWkwKrN"
      },
      "source": [
        "# Encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "e0vFU6TCDTx-"
      },
      "outputs": [],
      "source": [
        "def get_lstm_encoder(lstm_units, dropout, final_size):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.LayerNormalization(epsilon=1e-6, axis=-1))\n",
        "    \n",
        "    for dim in lstm_units:\n",
        "      model.add(layers.LSTM(dim, return_sequences= True))\n",
        "      model.add(layers.Dropout(dropout))\n",
        "    model.add(layers.LSTM(final_size))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "dQU857gELHrC"
      },
      "outputs": [],
      "source": [
        "def get_conv_encoder(conv_filters, kernel, dropout, hidden_activation):\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.LayerNormalization(epsilon=1e-6, axis=-1))\n",
        "\n",
        "  first_layer = conv_filters.pop(0)\n",
        "  model.add(layers.Conv1D(first_layer, kernel, activation = hidden_activation, padding = \"same\"))\n",
        "    \n",
        "  for filter in conv_filters:\n",
        "    model.add(layers.Dropout(dropout))\n",
        "    model.add(layers.Conv1D(filter, kernel, activation = hidden_activation, padding = \"same\"))\n",
        "    \n",
        "  model.add(layers.GlobalAveragePooling1D())\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "U_oF8CCXPWRx"
      },
      "outputs": [],
      "source": [
        "def get_feed_forward_encoder(units, dropout, final_size, hidden_activation):\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.LayerNormalization(epsilon=1e-6, axis=-1))\n",
        "\n",
        "  for unit in units:\n",
        "    model.add(layers.Dense(unit, activation = hidden_activation))\n",
        "    model.add(layers.Dropout(dropout))\n",
        "  model.add(layers.Dense(final_size, activation = \"softmax\"))\n",
        "  model.add(layers.GlobalAveragePooling1D())\n",
        "  #model.add(layers.Dense(final_size, activation = \"softmax\"))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duBk3NDdvv4j"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "DWfVb-K8vv4k"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "def apply_positional_encoding(x, position, d_model):\n",
        "  return x + positional_encoding(position, d_model)\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x) # Is this necesarry?\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Hl34vXvFvv4l"
      },
      "outputs": [],
      "source": [
        "def get_tunable_transformer(hp):\n",
        "    input_shape = (202,2)\n",
        "    n_classes = 5\n",
        "\n",
        "    # 2 tune\n",
        "    head_size = hp.Int('head_size', min_value = 32, max_value = 256, step = 32)\n",
        "    num_heads = hp.Int(\"num_heads\", min_value = 2, max_value = 8, step = 2 )\n",
        "    ff_dim = hp.Int('ff_dim', min_value = 2, max_value=8, step = 2)\n",
        "\n",
        "    num_transformer_blocks = hp.Int(\"num_transformer_block\", min_value = 1, max_value=4, step=1)\n",
        "    \n",
        "    mlp_dropout = hp.Float(\"dropout\", min_value = 0.0, max_value = 0.5)\n",
        "    mlp_units = [\n",
        "                hp.Choice(\"mlp_1\", values = [128, 0]),\n",
        "                hp.Choice(\"mlp_2\", values = [64, 0]),\n",
        "                hp.Choice(\"mlp_3\", values = [32, 0]),\n",
        "    ]    \n",
        "    dropout=hp.Float('dropout', min_value = 0., max_value=0.5, step=0.1)\n",
        "    \n",
        "    # building model\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    x = layers.LayerNormalization(epsilon=1e-6, axis=-2)(x)\n",
        "    x = apply_positional_encoding(x, input_shape[-2], input_shape[-1])\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "    optimizer = keras.optimizers.Adam(keras.optimizers.schedules.CosineDecay(initial_learning_rate=hp_learning_rate, decay_steps=1000, alpha=1e-2))\n",
        "    model.compile(\n",
        "      loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "      optimizer=optimizer,\n",
        "      metrics=[\"sparse_categorical_accuracy\"],\n",
        "    )\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65-nLEw1alO6"
      },
      "source": [
        "# Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "rMdnoPkwNToH"
      },
      "outputs": [],
      "source": [
        "# Classifier function\n",
        "def get_mlp_classifier(n_classes, mlp_dropout, dimensions):\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  for dim in dimensions:\n",
        "        model.add(layers.Dense(dim, activation=\"relu\"))\n",
        "        model.add(layers.Dropout(mlp_dropout))\n",
        "\n",
        "  model.add(layers.Dense(n_classes, activation=\"softmax\"))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DIK_fepP4Km"
      },
      "source": [
        "# Tuning builders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "6ZK4UoJFJeOj"
      },
      "outputs": [],
      "source": [
        "from itertools import chain, combinations\n",
        "\n",
        "def powerset(iterable):\n",
        "    s = list(iterable)\n",
        "    return list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))\n",
        "\n",
        "def get_powerset(option_list):\n",
        "  power = list(powerset(option_list))\n",
        "  resulting_set = []\n",
        "  for item in power:\n",
        "    resulting_set.append(list(item))\n",
        "  return resulting_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "fFU-jj2JM9Pa"
      },
      "outputs": [],
      "source": [
        "def get_tunable_classifier(hp):\n",
        "  n_classes = 5\n",
        "  mlp_dropout = hp.Float(\"dropout\", min_value = 0.0, max_value = 0.8)\n",
        "\n",
        "  mlp_units = [\n",
        "               hp.Choice(\"mlp_1\", values = [128, 0]),\n",
        "               hp.Choice(\"mlp_2\", values = [64, 0]),\n",
        "               hp.Choice(\"mlp_3\", values = [32, 0]),\n",
        "  ]\n",
        "\n",
        "  return get_mlp_classifier(n_classes, mlp_dropout, mlp_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "yZWVikQ8J8aW"
      },
      "outputs": [],
      "source": [
        "def get_model_built(model, learning_rate, alpha):\n",
        "  model.build(input_shape  = (None,2,202))\n",
        "  optimizer=keras.optimizers.Adam(learning_rate=keras.optimizers.schedules.CosineDecay(initial_learning_rate=learning_rate, decay_steps=1000, alpha=alpha))\n",
        "  model.compile(optimizer = optimizer,\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "                metrics=['sparse_categorical_accuracy'])\n",
        "  return model\n",
        "\n",
        "\n",
        "def get_tunable_lstm_builder(get_classifier_func):\n",
        "  def get_tunable_lstm(hp):\n",
        "    lstm_units = [\n",
        "                  hp.Choice(\"lstm_1\", values = [128, 0]),\n",
        "                  hp.Choice(\"lstm_2\", values = [64, 0]),\n",
        "                  hp.Choice(\"lstm_3\", values = [32, 0]),\n",
        "    ]\n",
        "    lstm_units = filter(lambda x: x != 0, lstm_units)\n",
        "\n",
        "    dropout = hp.Float(\"dropout\", min_value = 0.0, max_value = 0.5)\n",
        "    final_size = hp.Int(\"encoder output size\", min_value = 3, max_value = 100)\n",
        "\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "    hp_alpha = hp.Choice('alpha', values=[1e-1, 1e-2])\n",
        "\n",
        "    model = keras.Sequential([\n",
        "                              get_lstm_encoder(lstm_units, dropout, final_size),\n",
        "                              get_classifier_func(hp)   \n",
        "    ])\n",
        "    model = get_model_built(model, hp_learning_rate, hp_alpha)\n",
        "    return model\n",
        "  return get_tunable_lstm\n",
        "\n",
        "def get_tunable_conv_builder_mod(get_classifier_func):\n",
        "  def get_tunable_conv(hp):\n",
        "    filter_size = hp.Int(\"filter\", min_value = 32, max_value = 128)\n",
        "    filter_count = hp.Int(\"filter_count\", min_value = 1, max_value = 3)\n",
        "    conv_filter = [filter_size for _ in range(filter_count)]\n",
        "\n",
        "    kernel = hp.Int(\"kernel\", min_value = 1, max_value = 3)\n",
        "    dropout = hp.Float(\"dropout\", min_value = 0.0, max_value = 0.5)\n",
        "\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "    hp_alpha = hp.Choice('alpha', values=[1e-1, 1e-2])\n",
        "\n",
        "    hidden_activation = hp.Choice(\"hidden_activation\", values = [\"relu\", \"sigmoid\", \"gelu\"])\n",
        "\n",
        "    model = keras.Sequential([\n",
        "                              get_conv_encoder(conv_filter, kernel, dropout,hidden_activation),\n",
        "                              get_classifier_func(hp)   \n",
        "    ])\n",
        "    model = get_model_built(model, hp_learning_rate, hp_alpha)\n",
        "    return model\n",
        "  return get_tunable_conv\n",
        "\n",
        "\n",
        "def get_tunable_ff_builder(get_classifier_func):\n",
        "  def get_tunable_ff(hp):\n",
        "    ff_units = [\n",
        "                  hp.Choice(\"ff_1\", values = [128, 0]),\n",
        "                  hp.Choice(\"ff_2\", values = [64, 0]),\n",
        "                  hp.Choice(\"ff_3\", values = [32, 0]),              \n",
        "    ]\n",
        "    ff_units = filter(lambda x : x != 0, ff_units)\n",
        "\n",
        "    activation = hp.Choice(\"hidden activation\", values = [\"relu\",\"sigmoid\",\"gelu\"])\n",
        "    dropout = hp.Float(\"dropout\", min_value = 0.0, max_value = 0.5)\n",
        "    \n",
        "    final_size = hp.Int(\"encoder output size\", min_value = 3, max_value = 100)\n",
        "\n",
        "\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "    hp_alpha = hp.Choice('alpha', values=[1e-1, 1e-2])\n",
        "\n",
        "    model = keras.Sequential([\n",
        "                              get_feed_forward_encoder(ff_units, dropout,final_size, activation), \n",
        "                              get_classifier_func(hp)   \n",
        "    ])\n",
        "    model = get_model_built(model, hp_learning_rate, hp_alpha)\n",
        "    return model\n",
        "  return get_tunable_ff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7s-dEwKxqyO"
      },
      "source": [
        "# record stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "FgYn18noxuSU"
      },
      "outputs": [],
      "source": [
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "  if isinstance(value, type(tf.constant(0))):\n",
        "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "# Takes an array\n",
        "def _floatList_feature(value):\n",
        "  return tf.train.Feature(float_list = tf.train.FloatList(value = value.flatten()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "p4Ptml2wx33i"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "mode_to_index = {\n",
        "    0: 0, \n",
        "    1: 1, \n",
        "    3: 2, \n",
        "    4: 3, \n",
        "    6: 4\n",
        "}\n",
        "\n",
        "def diff(arr):\n",
        "    res = np.diff(arr)\n",
        "    return np.concatenate((arr[0:1], res))\n",
        "\n",
        "def array_lag(arr, n):\n",
        "    if n < 0:\n",
        "        return np.pad(arr, [0, -n])[-n:]\n",
        "    return np.pad(arr, [n, 0])[:-n]\n",
        "\n",
        "def add_features(df):\n",
        "    df_copy = df\n",
        "    \n",
        "    #df_copy['volume'] = df_copy['flow'].apply(volume_calc)\n",
        "\n",
        "    df_copy['flow_diff'] = df_copy['flow'].apply(diff)\n",
        "    df_copy['pressure_diff'] = df_copy['pressure'].apply(diff)\n",
        "\n",
        "    df_copy['flow_1lag'] = df_copy['flow'].apply(lambda x: array_lag(x, 1))\n",
        "    df_copy['flow_2lag'] = df_copy['flow'].apply(lambda x: array_lag(x, 2))\n",
        "    df_copy['flow_3lag'] = df_copy['flow'].apply(lambda x: array_lag(x, 3))\n",
        "    \n",
        "    df_copy['pressure_1lag'] = df_copy['pressure'].apply(lambda x: array_lag(x, 1))\n",
        "    df_copy['pressure_2lag'] = df_copy['pressure'].apply(lambda x: array_lag(x, 2))\n",
        "    df_copy['pressure_3lag'] = df_copy['pressure'].apply(lambda x: array_lag(x, 3))\n",
        "    \n",
        "    df_copy['flow_1oracle'] = df_copy['flow'].apply(lambda x: array_lag(x, -1))\n",
        "    df_copy['flow_2oracle'] = df_copy['flow'].apply(lambda x: array_lag(x, -2))\n",
        "    df_copy['flow_3oracle'] = df_copy['flow'].apply(lambda x: array_lag(x, -3))\n",
        "    \n",
        "    df_copy['pressure_1oracle'] = df_copy['pressure'].apply(lambda x: array_lag(x, -1))\n",
        "    df_copy['pressure_2oracle'] = df_copy['pressure'].apply(lambda x: array_lag(x, -2))\n",
        "    df_copy['pressure_3oracle'] = df_copy['pressure'].apply(lambda x: array_lag(x, -3))\n",
        "\n",
        "    df_copy['flow_cumsum'] = df_copy['flow'].apply(lambda x: np.cumsum(x))\n",
        "    df_copy['pressure_cumsum'] = df_copy['pressure'].apply(lambda x: np.cumsum(x))\n",
        "    \n",
        "    return df_copy\n",
        "\n",
        "def writeAsRecord(recordName, file_link):\n",
        "  data_csv = pd.read_csv(file_link)\n",
        "\n",
        "  data_np = data_csv[[\"flow\", \"pressure\", \"y\"]]\n",
        "  data_np[\"flow\"] = data_np[\"flow\"].apply(literal_eval) # The csv reader from pandas reads them as strings\n",
        "  data_np[\"pressure\"] = data_np[\"pressure\"].apply(literal_eval)\n",
        "  data_np = convert_to_fixed_length(data_np, 202)\n",
        "  #data_np = add_features(data_np)\n",
        "\n",
        "\n",
        "\n",
        "  # filtering classes\n",
        "  data_np = data_np[data_np['y'].isin(mode_to_index.keys())]\n",
        "  data_np['y'] = data_np['y'].apply(lambda x: mode_to_index[x])\n",
        "\n",
        "  #data_np = data_np.explode([\"flow\", \"pressure\"])\n",
        "  data_np = data_np.to_numpy()\n",
        "\n",
        "  # Writing rfRecord\n",
        "  with tf.io.TFRecordWriter(recordName + '.tfrecord') as tfrecord:\n",
        "    for idx in range(data_np.shape[0]):\n",
        "      feature_set = {\n",
        "      'label'              : _int64_feature(idx),      #tf.train.Feature(int64_list=tf.train.Int64List(value=label)),\n",
        "      'flow'               : _floatList_feature(np.array(data_np[idx][0])),\n",
        "      'pressure'           : _floatList_feature(np.array(data_np[idx][1])),\n",
        "      'y'                  : _float_feature(data_np[idx][2]), #tf.train.Feature(int64_list= tf.train.FloatList(value = feature))\n",
        "      #'volume'             : _floatList_feature(np.array(data_np[idx][3])),\n",
        "      #'flow_diff'          : _floatList_feature(np.array(data_np[idx][3])),\n",
        "      #'pressure_diff'      : _floatList_feature(np.array(data_np[idx][4])),\n",
        "      #'flow_1lag'          : _floatList_feature(np.array(data_np[idx][5])),\n",
        "      #'flow_2lag'          : _floatList_feature(np.array(data_np[idx][6])),\n",
        "      #'flow_3lag'          : _floatList_feature(np.array(data_np[idx][7])),\n",
        "      #'pressure_1lag'      : _floatList_feature(np.array(data_np[idx][8])),\n",
        "      #'pressure_2lag'      : _floatList_feature(np.array(data_np[idx][9])),\n",
        "      #'pressure_3lag'      : _floatList_feature(np.array(data_np[idx][10])),\n",
        "      #'flow_1oracle'       : _floatList_feature(np.array(data_np[idx][11])),\n",
        "      #'flow_2oracle'       : _floatList_feature(np.array(data_np[idx][12])),\n",
        "      #'flow_3oracle'       : _floatList_feature(np.array(data_np[idx][13])),\n",
        "      #'pressure_1oracle'   : _floatList_feature(np.array(data_np[idx][14])),\n",
        "      #'pressure_2oracle'   : _floatList_feature(np.array(data_np[idx][15])),\n",
        "      #'pressure_3oracle'   : _floatList_feature(np.array(data_np[idx][16])),\n",
        "      #'flow_cumsum'        : _floatList_feature(np.array(data_np[idx][17])),\n",
        "      #'pressure_cumsum'   : _floatList_feature(np.array(data_np[idx][18])),\n",
        "\n",
        "      }\n",
        "\n",
        "      example = tf.train.Example(features = tf.train.Features(feature=feature_set))\n",
        "      tfrecord.write(example.SerializeToString())\n",
        "\n",
        "# Writing tfRecords\n",
        "train_file_link = f\"https://drive.google.com/u/0/uc?id=1pHC5l8ww5CnlWpI3RMD7XJAm0lhTMusj&export=download&confirm=t\"\n",
        "test_file_link = f\"https://drive.google.com/u/1/uc?id=1hTn-st0A4ZsOPLpHgVilKOgk3IhPnmai&export=download&confirm=t\"\n",
        "\n",
        "writeAsRecord(\"train_raw\", train_file_link)\n",
        "writeAsRecord(\"test_raw\", test_file_link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "M2Xx65LlSJWD"
      },
      "outputs": [],
      "source": [
        "# defining features of record\n",
        "@tf.function \n",
        "def map_fn(serialized_example):\n",
        "    feature = {\n",
        "        'label'                 : tf.io.FixedLenFeature(1, tf.int64),\n",
        "        'flow'                  : tf.io.FixedLenFeature(202, tf.float32),\n",
        "        'pressure'              : tf.io.FixedLenFeature(202, tf.float32),\n",
        "        'y'                     : tf.io.FixedLenFeature(1, tf.float32),\n",
        "        #'volume'                : tf.io.FixedLenFeature(200, tf.float32),\n",
        "        #'flow_diff'             : tf.io.FixedLenFeature(200, tf.float32),\n",
        "        #'pressure_diff'         : tf.io.FixedLenFeature(200, tf.float32),\n",
        "        #'flow_1lag'             : tf.io.FixedLenFeature(200, tf.float32),\n",
        "        #'flow_2lag'             : tf.io.FixedLenFeature(200, tf.float32),\n",
        "        #'flow_3lag'             : tf.io.FixedLenFeature(200, tf.float32),\n",
        "        #'pressure_1lag'         : tf.io.FixedLenFeature(200, tf.float32),\n",
        "        #'pressure_2lag'         : tf.io.FixedLenFeature(200, tf.float32),\n",
        "        #'pressure_3lag'         : tf.io.FixedLenFeature(200, tf.float32),\n",
        "        #'flow_1oracle'          : tf.io.FixedLenFeature(200, tf.float32),\n",
        "        #'flow_2oracle'          : tf.io.FixedLenFeature(200, tf.float32),\n",
        "        #'flow_3oracle'          : tf.io.FixedLenFeature(200, tf.float32),\n",
        "        #'pressure_1oracle'      : tf.io.FixedLenFeature(200, tf.float32),\n",
        "        #'pressure_2oracle'      : tf.io.FixedLenFeature(200, tf.float32),\n",
        "        #'pressure_3oracle'      : tf.io.FixedLenFeature(200, tf.float32),\n",
        "        #'flow_cumsum'           : tf.io.FixedLenFeature(200, tf.float32),\n",
        "        #'pressure_cumsum'       : tf.io.FixedLenFeature(200, tf.float32),        \n",
        "    }\n",
        "    return tf.io.parse_single_example(serialized_example, feature)\n",
        "\n",
        "@tf.function \n",
        "def map_all(serialized):\n",
        "    ex = map_fn(serialized)\n",
        "    return (tf.stack([ex[\"flow\"], ex[\"pressure\"]],1), ex[\"y\"]) \n",
        "                        #ex['volume'], \n",
        "                        #ex['flow_diff'], ex['pressure_diff'], \n",
        "                        #ex['flow_1lag'], ex['flow_2lag'], ex['flow_3lag'], \n",
        "                        #ex['pressure_1lag'], ex['pressure_2lag'], ex['pressure_3lag'], \n",
        "                        #ex['flow_1oracle'], ex['flow_2oracle'], ex['flow_3oracle'], \n",
        "                        #ex['pressure_1oracle'], ex['pressure_2oracle'], ex['pressure_3oracle'], ex['flow_cumsum'], ex['pressure_cumsum']], 1), ex[\"y\"])\n",
        "  \n",
        "\n",
        "@tf.function\n",
        "def get_dist(targets, n_classes, window_size):\n",
        "  shape = (n_classes,)\n",
        "  res = tf.zeros(shape)\n",
        "\n",
        "  for i in range(window_size):\n",
        "    idx = tf.gather(targets, i, axis=0)\n",
        "    mat = tf.sparse.SparseTensor([[idx]], [1.], shape)\n",
        "    res = tf.sparse.add(res, mat)\n",
        "    \n",
        "  res = tf.math.divide(res, window_size)\n",
        "  return tf.ensure_shape(res, shape)\n",
        "\n",
        "@tf.function\n",
        "def map_window(ds, window_size, input_shape):\n",
        "  feat = tf.zeros([0] + list(input_shape))\n",
        "  y = tf.zeros([0, ])\n",
        "  \n",
        "  for sample in ds:\n",
        "    ex = map_all(sample)\n",
        "\n",
        "    features = ex[0]\n",
        "    features = tf.convert_to_tensor(features)\n",
        "    features = tf.expand_dims(features, axis=0)\n",
        "\n",
        "    feat = tf.concat([feat, features], axis=0)\n",
        "    y = tf.concat([y, ex[1]], axis=0)\n",
        "    \n",
        "  feat = tf.ensure_shape(feat, (window_size, 200, 17))\n",
        "  y = tf.ensure_shape(y, (window_size,))\n",
        "  return (feat, y)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "eqC94yVMy6Qa"
      },
      "outputs": [],
      "source": [
        "n_classes = 5\n",
        "calc_dist = lambda x, y: (x, get_dist(y, n_classes, window_size))\n",
        "\n",
        "train_set = tf.data.TFRecordDataset('train_raw.tfrecord').shard(2, 0)\n",
        "val_set = tf.data.TFRecordDataset('train_raw.tfrecord').shard(2, 1)\n",
        "train_set = train_set.map(map_all)\n",
        "val_set = val_set.map(map_all)\n",
        "\n",
        "test_dataset = tf.data.TFRecordDataset('test_raw.tfrecord')\n",
        "test_dataset = test_dataset.map(map_all)\n",
        "\n",
        "\n",
        "train_size = sum(1 for _ in train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYpbYh5NaPiQ",
        "outputId": "02bbd066-8213-4dac-e527-5d692be5e901"
      },
      "outputs": [],
      "source": [
        "# transpose \n",
        "\n",
        "transpose_set = train_set.map(lambda x,y : (tf.transpose(x),y))\n",
        "transpose_set = transpose_set.shuffle(10000)\n",
        "transpose_set = transpose_set.repeat()\n",
        "\n",
        "transpose_val_set = val_set.map(lambda x,y : (tf.transpose(x),y))\n",
        "\n",
        "\n",
        "transpose_test_set = test_dataset.map(lambda x,y : (tf.transpose(x),y))\n",
        "\n",
        "train_set = train_set.shuffle(10000)\n",
        "train_set = train_set.repeat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfuhz88aJhGE"
      },
      "source": [
        "# Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "PMEfyMT1gtHr"
      },
      "outputs": [],
      "source": [
        "def tune_model(get_model_func, epochs, batch_size, directory, name):\n",
        "  tuner = kt.Hyperband(\n",
        "      get_model_func,\n",
        "      objective='val_loss',\n",
        "      directory=directory,\n",
        "      project_name= name,\n",
        "      hyperband_iterations=1,\n",
        "      #max_epochs=50\n",
        "  )\n",
        "\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "  tuner.search(transpose_set.batch(batch_size), epochs = epochs, steps_per_epoch = train_size // batch_size, validation_data = transpose_val_set.batch(batch_size), callbacks = [stop_early])\n",
        "  best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "  print(best_hps.values)\n",
        "  \n",
        "  return tuner.hypermodel.build(best_hps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "e74TWTv84Spf"
      },
      "outputs": [],
      "source": [
        "def tune_model_not_transposed(get_model_func, epochs, batch_size, directory, name):\n",
        "  tuner = kt.Hyperband(\n",
        "      get_model_func,\n",
        "      objective='val_loss',\n",
        "      directory=directory,\n",
        "      project_name= name\n",
        "  )\n",
        "\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "  tuner.search(train_set.batch(batch_size), epochs = epochs, steps_per_epoch = train_size // batch_size, validation_data = val_set.batch(batch_size), callbacks = [stop_early])\n",
        "  best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "  print(best_hps.values)\n",
        "  \n",
        "  return tuner.hypermodel.build(best_hps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OtrVOnTMgUf"
      },
      "source": [
        "# tuning and fitting for report data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning lstm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        },
        "id": "lqnlHmWDit6-",
        "outputId": "6e4c2979-b980-44fc-dc0b-f0fe7d008c91"
      },
      "outputs": [],
      "source": [
        "# LSTM\n",
        "\n",
        "epochs = 50\n",
        "batch = 1048\n",
        "\n",
        "tuned_lstm = tune_model(\n",
        "    get_tunable_lstm_builder(get_tunable_classifier),\n",
        "    epochs,\n",
        "    batch, \"lstm_cross2\", \"lstm_cross2\"\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "tuned_lstm.fit(\n",
        "    transpose_set.batch(batch),\n",
        "    epochs = epochs,\n",
        "    steps_per_epoch = train_size // batch,\n",
        "    validation_data = transpose_val_set.batch(batch),\n",
        "    callbacks = [stop_early]\n",
        "    )\n",
        "\n",
        "model_json = tuned_lstm.to_json()\n",
        "\n",
        "# Writing json file\n",
        "with open(\"lstm_model_cross2.json\",\"w\") as json_file:\n",
        "  json_file.write(model_json)\n",
        "\n",
        "tuned_lstm.save_weights(\"lstm_model_cross2.h5\")\n",
        "\n",
        "print(\"model saved\")\n",
        "\n",
        "tuned_lstm.evaluate(transpose_test_set.batch(batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning CONV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LX4nxTAZJF_X",
        "outputId": "a0de0e10-df48-49c2-c6ff-7d29287acad2"
      },
      "outputs": [],
      "source": [
        "# CONV\n",
        "\n",
        "epochs = 50\n",
        "batch = 1048\n",
        "\n",
        "tuned_conv = tune_model(\n",
        "    get_tunable_conv_builder_mod(get_tunable_classifier),\n",
        "    epochs, \n",
        "    batch, \"conv_cross2\", \"conv_cross2\"\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "tuned_conv.fit(\n",
        "      transpose_set.batch(batch),\n",
        "      epochs = epochs,\n",
        "      steps_per_epoch = train_size // batch,\n",
        "      validation_data = transpose_val_set.batch(batch),\n",
        "      callbacks = [stop_early]\n",
        "    )\n",
        "\n",
        "model_json = tuned_conv.to_json()\n",
        "\n",
        "# Writing json file\n",
        "with open(\"conv_model_cross2.json\",\"w\") as json_file:\n",
        "  json_file.write(model_json)\n",
        "\n",
        "tuned_conv.save_weights(\"conv_model_cross2.h5\")\n",
        "\n",
        "print(\"model saved\")\n",
        "\n",
        "tuned_conv.evaluate(transpose_test_set.batch(batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning FF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdRl5_gVJUQH",
        "outputId": "52381d37-a433-4f26-8d2c-d20fd6855c3a"
      },
      "outputs": [],
      "source": [
        "# FEED FORWARD\n",
        "\n",
        "epochs = 50\n",
        "batch = 1048\n",
        "\n",
        "tuned_ff = tune_model(\n",
        "    get_tunable_ff_builder(get_tunable_classifier),\n",
        "    epochs, \n",
        "    batch, \"ff_cross2\", \"ff_cross2\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "tuned_ff.fit(\n",
        "      transpose_set.batch(batch),\n",
        "      epochs = epochs,\n",
        "      steps_per_epoch = train_size // batch,\n",
        "      validation_data = transpose_val_set.batch(batch),\n",
        "      callbacks = [stop_early]\n",
        "    )\n",
        "\n",
        "model_json = tuned_ff.to_json()\n",
        "\n",
        "# Writing json file\n",
        "with open(\"ff_model_cross2.json\",\"w\") as json_file:\n",
        "  json_file.write(model_json)\n",
        "\n",
        "tuned_ff.save_weights(\"ff_model_cross2.h5\")\n",
        "print(\"model saved\")\n",
        "\n",
        "tuned_ff.evaluate(transpose_test_set.batch(batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "3FeMjyAP1xp8",
        "outputId": "14235491-7a8d-4885-b25b-f9375df6bee6"
      },
      "outputs": [],
      "source": [
        "# transformer not made to run with transposed data\n",
        "\n",
        "epochs = 50\n",
        "batch = 128\n",
        "\n",
        "transformer = tune_model_not_transposed(\n",
        "    get_tunable_transformer,\n",
        "    epochs, \n",
        "    batch, \"transformer_cross2\", \"transformer_cross2\"\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test\n",
        "tuner = kt.Hyperband(\n",
        "    get_tunable_transformer,\n",
        "    objective='val_loss',\n",
        "    directory=\"transformer_cross2\",\n",
        "    project_name= \"transformer_cross2\"\n",
        ")\n",
        "\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(best_hps.values)\n",
        "  \n",
        "transformer = tuner.hypermodel.build(best_hps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 100 #test\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "transformer.fit(\n",
        "      train_set.batch(batch),\n",
        "      epochs = epochs,\n",
        "      steps_per_epoch = train_size // batch,\n",
        "      validation_data = val_set.batch(batch),\n",
        "      callbacks = [stop_early]\n",
        "    )\n",
        "\n",
        "model_json = transformer.to_json()\n",
        "\n",
        "# Writing json file\n",
        "with open(\"transformer_cross2.json\",\"w\") as json_file:\n",
        "  json_file.write(model_json)\n",
        "\n",
        "transformer.save_weights(\"transformer_cross2.h5\")\n",
        "print(\"model saved\")\n",
        "\n",
        "transformer.evaluate(test_dataset.batch(batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EsT3L3pJtVG"
      },
      "source": [
        "# Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "dCsrZPciJu10"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def f1_evaluation(file_no_ext, test_set):\n",
        "    json_file = open(file_no_ext + \".json\", 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = keras.models.model_from_json(loaded_model_json)\n",
        "    # load weights into new model\n",
        "    loaded_model.load_weights(file_no_ext + \".h5\")\n",
        "\n",
        "    y_pred = [np.argmax(y) for y in loaded_model.predict(test_set.batch(128))]\n",
        "    y_true = list(test_set.map(lambda _, y: y[0]))\n",
        "    \n",
        "    print(file_no_ext)\n",
        "    print(metrics.classification_report(y_true,y_pred))\n",
        "    print(metrics.confusion_matrix(y_true, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f1_evaluation(\"lstm_model_cross2\", transpose_test_set)\n",
        "f1_evaluation(\"conv_model_cross2\", transpose_test_set)\n",
        "f1_evaluation(\"ff_model_cross2\", transpose_test_set)\n",
        "f1_evaluation(\"transformer_cross2\", test_dataset)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Encoders_raw.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "687c238b3ce093b6d05593c14198ba6d4a27c356ca33aff629c1e2e7517a29f8"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
